# Do modelo ao produto: como uma API de LLM vira funcionalidade em um app de entrega

![Capa: Do modelo ao produto](../img/artigo_1/capa.png)

VocÃª jÃ¡ parou para pensar por que grandes apps de delivery parecem "inteligentes" ao resumir avaliaÃ§Ãµes de restaurantes? NÃ£o Ã© mÃ¡gica â€” Ã© engenharia sÃ³lida por trÃ¡s de um LLM simples. Neste post, vou desmistificar essa jornada: de um protÃ³tipo rÃ¡pido em Jupyter para uma feature robusta em produÃ§Ã£o, lidando com latÃªncia, custos e escalabilidade. Como dev full stack e DevOps, vi isso na prÃ¡tica em apps de alto trÃ¡fego. Vamos mergulhar?

![Pipeline de LLM em produÃ§Ã£o](../img/artigo_1/figura1.png)
_Figura 1: Pipeline de LLM em produÃ§Ã£o: de protÃ³tipo a feature escalÃ¡vel_

## 1. Contexto e PropÃ³sito: Por que isso importa no mundo real?

Nos Ãºltimos anos, integrar LLMs virou algo trivial: gere uma chave de API, escreva poucas linhas de cÃ³digo e _voilÃ _ â€” respostas "inteligentes". Mas em um app de delivery processando milhares de pedidos por minuto, isso Ã© sÃ³ o comeÃ§o.

O grande desafio? **Transformar essa capacidade bruta em valor real para o usuÃ¡rio**, sem quebrar o app com timeouts ou custos exorbitantes. Aqui, compartilho a jornada de protÃ³tipo para produÃ§Ã£o, focando em desafios de engenharia que todo dev precisa dominar.

> **Por que ler isso?** Se vocÃª constrÃ³i features com IA, vai evitar armadilhas comuns que eu jÃ¡ cometi â€” e escalar de forma sustentÃ¡vel.

## 2. Abordagem: Um caso prÃ¡tico em apps de delivery

Para tornar concreto, vamos usar um exemplo clÃ¡ssico: **resumir avaliaÃ§Ãµes de restaurantes**. Em vez de listar dezenas de reviews, o app gera um parÃ¡grafo imparcial destacando prÃ³s e contras.

**Estrutura do post:**

- **Conceitos bÃ¡sicos**: DiferenÃ§a entre modelo e feature.
- **Arquitetura**: Fluxo de dados do app ao LLM.
- **CÃ³digo prÃ¡tico**: ImplementaÃ§Ã£o em Python/FastAPI.
- **ProduÃ§Ã£o**: MÃ©tricas, riscos e soluÃ§Ãµes reais.

## 3. Conceitos Fundamentais: Alinhando o vocabulÃ¡rio

Antes de codar, vamos alinhar termos para evitar confusÃµes.

| Termo              | DefiniÃ§Ã£o                                                 | Contexto de Delivery                        |
| :----------------- | :-------------------------------------------------------- | :------------------------------------------ |
| **O Modelo (LLM)** | O "cÃ©rebro" (GPT-4, Llama 3). Stateless e probabilÃ­stico. | A engine que processa o texto.              |
| **A Feature**      | O pacote completo.                                        | Inclui busca no DB, prompts, API e UI.      |
| **Context Window** | Limite de tokens processados de uma vez.                  | Foque nas Ãºltimas 20 avaliaÃ§Ãµes para caber. |

> **MudanÃ§a de mindset chave**: Pare de tratar o LLM como orÃ¡culo infalÃ­vel. Veja-o como um **transformador de texto** em um pipeline clÃ¡ssico de software.

## 4. MÃ£o na Massa: Exemplo PrÃ¡tico - Implementando o endpoint

Agora, o prÃ¡tico: crie um endpoint GET /restaurants/{id}/reviews/summary. Ele pega as Ãºltimas 20 reviews textuais e retorna um resumo conciso.

O Fluxo Passo a Passo
App pede o resumo (ex: ao abrir a pÃ¡gina do restaurante).
Backend consulta o banco (Postgres ou DynamoDB).
Backend constrÃ³i o prompt com as reviews.
Backend chama o LLM via API.
Backend formata a resposta e devolve ao app.
CÃ³digo Exemplo (Python com FastAPI)
Aqui vai um snippet funcional â€” teste no seu ambiente local:

```python
import os
from typing import List
from fastapi import FastAPI, HTTPException
from openai import OpenAI

app = FastAPI()
# Certifique-se de ter a variÃ¡vel de ambiente OPENAI_API_KEY configurada
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Simula busca no banco (em prod: use SQLAlchemy ou similar)
def get_recent_reviews(restaurant_id: str) -> List[str]:
    return [
        "A comida chegou fria, mas estava saborosa.",
        "Entrega super rÃ¡pida! O sushi estava fresco.",
        "Demorou muito e o entregador foi rude.",
        "Melhor hambÃºrguer da cidade, recomendo o de cheddar."
    ]

def generate_summary_prompt(reviews: List[str]) -> str:
    reviews_text = "\n".join([f"- {r}" for r in reviews])
    return f"""
    VocÃª Ã© um assistente Ãºtil de um app de delivery.
    Analise as seguintes avaliaÃ§Ãµes de um restaurante e gere um resumo curto (max 3 linhas)
    destacando os pontos positivos e negativos. Seja imparcial.

    AvaliaÃ§Ãµes:
    {reviews_text}

    Resumo:
    """

@app.get("/restaurants/{restaurant_id}/reviews/summary")
async def get_review_summary(restaurant_id: str):
    try:
        reviews = get_recent_reviews(restaurant_id)

        if not reviews:
            return {"summary": "Este restaurante ainda nÃ£o possui avaliaÃ§Ãµes suficientes."}

        prompt = generate_summary_prompt(reviews)

        # Chama LLM (em prod: use async e retry logic)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.5  # ConsistÃªncia > criatividade
        )

        summary = response.choices[0].message.content.strip()

        return {
            "restaurant_id": restaurant_id,
            "summary": summary,
            "source_count": len(reviews)
        }

    except Exception as e:
        print(f"Erro ao gerar resumo: {e}")  # Log real em prod
        raise HTTPException(status_code=503, detail="Resumo indisponÃ­vel no momento.")
```

> [!TIP] > **Dica de Performance**: Em produÃ§Ã£o, troque por chamadas assÃ­ncronas (`aiohttp`) para nÃ£o bloquear o endpoint.

![Fluxo de resumo de reviews](../img/artigo_1/figura2.png)
_Figura 2: Fluxo de resumo de reviews em app de delivery_

## 5. MÃ©tricas, Riscos e Boas PrÃ¡ticas: Lidando com LatÃªncia, Custos e Falhas

CÃ³digo pronto? Ã“timo, mas em escala, LLMs sÃ£o traiÃ§oeiros. Aqui vÃ£o os pilares para tornar sua feature _production-ready_:

![Desafios de produÃ§Ã£o](../img/artigo_1/figura3.png)
_Figura 3: Desafios de produÃ§Ã£o_

| Desafio               | Problema                               | SoluÃ§Ã£o de Engenharia                                                                          |
| :-------------------- | :------------------------------------- | :--------------------------------------------------------------------------------------------- |
| **LatÃªncia** â±ï¸       | Chamadas de 2-10s geram abandono.      | **Cache (Redis)**: Gere resumos em batch ou cacheie por 24h. Leitura em ms.                    |
| **Custos** ğŸ’¸         | Milhares de chamadas explodem a conta. | **Rate Limiting**: Limite por restaurante. Use modelos menores (GPT-3.5) para tarefas simples. |
| **Confiabilidade** ğŸ›¡ï¸ | API cai ou alucina.                    | **Circuit Breaker**: Se falhar, retorne reviews originais. O app nunca quebra.                 |

> [!NOTE]
> Essas prÃ¡ticas vÃªm da engenharia tradicional â€” IA Ã© sÃ³ mais um componente.

## 6. Evidence & Exploration: Medindo o Impacto

NÃ£o confie sÃ³ no "parece bom". Teste de verdade:

![MÃ©tricas de impacto](../img/artigo_1/figura4.png)
_Figura 4: MÃ©tricas de impacto_

- **A/B Testing**: 50% dos users veem resumo; 50% veem lista raw. MÃ©trica? Aumento em pedidos ou tempo de decisÃ£o.
- **Feedback Loop**: BotÃµes "Ãštil/NÃ£o Ãºtil" no app. Use isso para refinar prompts (human-in-the-loop).
- **MÃ©tricas Chave**: Taxa de engajamento, custo por feature, taxa de erro.

> Em projetos reais, isso dobrou a retenÃ§Ã£o em telas de restaurante.

## 7. ReflexÃµes Pessoais & PrÃ³ximos Passos: LiÃ§Ãµes de Quem JÃ¡ Escalou Isso

Como dev full stack e DevOps, aprendi que 90% do sucesso com IA Ã© engenharia clÃ¡ssica: cache, monitoramento, resiliÃªncia. O prompt Ã© o tempero, mas sem base sÃ³lida, nÃ£o escala.

**PrÃ³ximos passos no meu radar:**

- **Para 10k+ reviews**: Integre RAG (Retrieval-Augmented Generation) â€” tema para outro post.
- **Combatendo alucinaÃ§Ãµes**: Guardrails e validaÃ§Ãµes â€” vem por aÃ­.

> E vocÃª, jÃ¡ integrou LLMs em apps de produÃ§Ã£o? Qual foi o maior desafio com latÃªncia ou custos? Compartilhe nos comentÃ¡rios â€” sua experiÃªncia pode ajudar outros devs a evitar dores de cabeÃ§a!
