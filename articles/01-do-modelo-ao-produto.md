# Do modelo ao produto: como uma API de LLM vira funcionalidade em um app de entrega

<div align="center">
  <img src="../img/artigo_1/capa.png" alt="Capa: Do modelo ao produto" width="70%">
</div>

Voc√™ j√° parou para pensar por que grandes apps de delivery parecem "inteligentes" ao resumir avalia√ß√µes de restaurantes? N√£o √© m√°gica ‚Äî √© engenharia s√≥lida por tr√°s de um LLM simples. Neste post, vou desmistificar essa jornada: de um prot√≥tipo r√°pido em Jupyter para uma feature robusta em produ√ß√£o, lidando com lat√™ncia, custos e escalabilidade. Como dev full stack e DevOps, vi isso na pr√°tica em apps de alto tr√°fego. Vamos mergulhar?

<div align="center">
  <img src="../img/artigo_1/figura1.png" alt="Pipeline de LLM em produ√ß√£o" width="70%">
  <p><em>Figura 1: Pipeline de LLM em produ√ß√£o: de prot√≥tipo a feature escal√°vel</em></p>
</div>

## 1. Contexto e Prop√≥sito: Por que isso importa no mundo real?

Nos √∫ltimos anos, integrar LLMs virou algo trivial: gere uma chave de API, escreva poucas linhas de c√≥digo e _voil√†_ ‚Äî respostas "inteligentes". Mas em um app de delivery processando milhares de pedidos por minuto, isso √© s√≥ o come√ßo.

O grande desafio? **Transformar essa capacidade bruta em valor real para o usu√°rio**, sem quebrar o app com timeouts ou custos exorbitantes. Aqui, compartilho a jornada de prot√≥tipo para produ√ß√£o, focando em desafios de engenharia que todo dev precisa dominar.

> **Por que ler isso?** Se voc√™ constr√≥i features com IA, vai evitar armadilhas comuns que eu j√° cometi ‚Äî e escalar de forma sustent√°vel.

## 2. Abordagem: Um caso pr√°tico em apps de delivery

Para tornar concreto, vamos usar um exemplo cl√°ssico: **resumir avalia√ß√µes de restaurantes**. Em vez de listar dezenas de reviews, o app gera um par√°grafo imparcial destacando pr√≥s e contras.

**Estrutura do post:**

- **Conceitos b√°sicos**: Diferen√ßa entre modelo e feature.
- **Arquitetura**: Fluxo de dados do app ao LLM.
- **C√≥digo pr√°tico**: Implementa√ß√£o em Python/FastAPI.
- **Produ√ß√£o**: M√©tricas, riscos e solu√ß√µes reais.

## 3. Conceitos Fundamentais: Alinhando o vocabul√°rio

Antes de codar, vamos alinhar termos para evitar confus√µes.

| Termo              | Defini√ß√£o                                                 | Contexto de Delivery                        |
| :----------------- | :-------------------------------------------------------- | :------------------------------------------ |
| **O Modelo (LLM)** | O "c√©rebro" (GPT-4, Llama 3). Stateless e probabil√≠stico. | A engine que processa o texto.              |
| **A Feature**      | O pacote completo.                                        | Inclui busca no DB, prompts, API e UI.      |
| **Context Window** | Limite de tokens processados de uma vez.                  | Foque nas √∫ltimas 20 avalia√ß√µes para caber. |

> **Mudan√ßa de mindset chave**: Pare de tratar o LLM como or√°culo infal√≠vel. Veja-o como um **transformador de texto** em um pipeline cl√°ssico de software.

## 4. M√£o na Massa: Exemplo Pr√°tico - Implementando o endpoint

Agora, o pr√°tico: crie um endpoint GET /restaurants/{id}/reviews/summary. Ele pega as √∫ltimas 20 reviews textuais e retorna um resumo conciso.

O Fluxo Passo a Passo
App pede o resumo (ex: ao abrir a p√°gina do restaurante).
Backend consulta o banco (Postgres ou DynamoDB).
Backend constr√≥i o prompt com as reviews.
Backend chama o LLM via API.
Backend formata a resposta e devolve ao app.
C√≥digo Exemplo (Python com FastAPI)
Aqui vai um snippet funcional ‚Äî teste no seu ambiente local:

```python
import os
from typing import List
from fastapi import FastAPI, HTTPException
from openai import OpenAI

app = FastAPI()
# Certifique-se de ter a vari√°vel de ambiente OPENAI_API_KEY configurada
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Simula busca no banco (em prod: use SQLAlchemy ou similar)
def get_recent_reviews(restaurant_id: str) -> List[str]:
    return [
        "A comida chegou fria, mas estava saborosa.",
        "Entrega super r√°pida! O sushi estava fresco.",
        "Demorou muito e o entregador foi rude.",
        "Melhor hamb√∫rguer da cidade, recomendo o de cheddar."
    ]

def generate_summary_prompt(reviews: List[str]) -> str:
    reviews_text = "\n".join([f"- {r}" for r in reviews])
    return f"""
    Voc√™ √© um assistente √∫til de um app de delivery.
    Analise as seguintes avalia√ß√µes de um restaurante e gere um resumo curto (max 3 linhas)
    destacando os pontos positivos e negativos. Seja imparcial.

    Avalia√ß√µes:
    {reviews_text}

    Resumo:
    """

@app.get("/restaurants/{restaurant_id}/reviews/summary")
async def get_review_summary(restaurant_id: str):
    try:
        reviews = get_recent_reviews(restaurant_id)

        if not reviews:
            return {"summary": "Este restaurante ainda n√£o possui avalia√ß√µes suficientes."}

        prompt = generate_summary_prompt(reviews)

        # Chama LLM (em prod: use async e retry logic)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.5  # Consist√™ncia > criatividade
        )

        summary = response.choices[0].message.content.strip()

        return {
            "restaurant_id": restaurant_id,
            "summary": summary,
            "source_count": len(reviews)
        }

    except Exception as e:
        print(f"Erro ao gerar resumo: {e}")  # Log real em prod
        raise HTTPException(status_code=503, detail="Resumo indispon√≠vel no momento.")
```

> [!TIP] > **Dica de Performance**: Em produ√ß√£o, troque por chamadas ass√≠ncronas (`aiohttp`) para n√£o bloquear o endpoint.

<div align="center">
  <img src="../img/artigo_1/figura2.png" alt="Fluxo de resumo de reviews" width="70%">
  <p><em>Figura 2: Fluxo de resumo de reviews em app de delivery</em></p>
</div>

## 5. M√©tricas, Riscos e Boas Pr√°ticas: Lidando com Lat√™ncia, Custos e Falhas

C√≥digo pronto? √ìtimo, mas em escala, LLMs s√£o trai√ßoeiros. Aqui v√£o os pilares para tornar sua feature _production-ready_:

<div align="center">
  <img src="../img/artigo_1/figura3.png" alt="Desafios de produ√ß√£o" width="70%">
  <p><em>Figura 3: Desafios de produ√ß√£o</em></p>
</div>

| Desafio               | Problema                               | Solu√ß√£o de Engenharia                                                                          |
| :-------------------- | :------------------------------------- | :--------------------------------------------------------------------------------------------- |
| **Lat√™ncia** ‚è±Ô∏è       | Chamadas de 2-10s geram abandono.      | **Cache (Redis)**: Gere resumos em batch ou cacheie por 24h. Leitura em ms.                    |
| **Custos** üí∏         | Milhares de chamadas explodem a conta. | **Rate Limiting**: Limite por restaurante. Use modelos menores (GPT-3.5) para tarefas simples. |
| **Confiabilidade** üõ°Ô∏è | API cai ou alucina.                    | **Circuit Breaker**: Se falhar, retorne reviews originais. O app nunca quebra.                 |

> [!NOTE]
> Essas pr√°ticas v√™m da engenharia tradicional ‚Äî IA √© s√≥ mais um componente.

## 6. Evidence & Exploration: Medindo o Impacto

N√£o confie s√≥ no "parece bom". Teste de verdade:

<div align="center">
  <img src="../img/artigo_1/figura4.png" alt="M√©tricas de impacto" width="70%">
  <p><em>Figura 4: M√©tricas de impacto</em></p>
</div>

- **A/B Testing**: 50% dos users veem resumo; 50% veem lista raw. M√©trica? Aumento em pedidos ou tempo de decis√£o.
- **Feedback Loop**: Bot√µes "√ötil/N√£o √∫til" no app. Use isso para refinar prompts (human-in-the-loop).
- **M√©tricas Chave**: Taxa de engajamento, custo por feature, taxa de erro.

> Em projetos reais, isso dobrou a reten√ß√£o em telas de restaurante.

## 7. Reflex√µes Pessoais & Pr√≥ximos Passos: Li√ß√µes de Quem J√° Escalou Isso

Como dev full stack e DevOps, aprendi que 90% do sucesso com IA √© engenharia cl√°ssica: cache, monitoramento, resili√™ncia. O prompt √© o tempero, mas sem base s√≥lida, n√£o escala.

**Pr√≥ximos passos no meu radar:**

- **Para 10k+ reviews**: Integre RAG (Retrieval-Augmented Generation) ‚Äî tema para outro post.
- **Combatendo alucina√ß√µes**: Guardrails e valida√ß√µes ‚Äî vem por a√≠.

> E voc√™, j√° integrou LLMs em apps de produ√ß√£o? Qual foi o maior desafio com lat√™ncia ou custos? Compartilhe nos coment√°rios ‚Äî sua experi√™ncia pode ajudar outros devs a evitar dores de cabe√ßa!
