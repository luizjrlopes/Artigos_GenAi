<!DOCTYPE html>

<html class="scroll-smooth" lang="pt-BR">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Do Modelo ao Produto: LLM em Apps de Entrega</title>
<script src="https://cdn.tailwindcss.com"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet"/>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&amp;family=Fira+Code:wght@400;500&amp;display=swap" rel="stylesheet"/>
<script>
      tailwind.config = {
        theme: {
          extend: {
            animation: {
                'spin-slow': 'spin 3s linear infinite',
                'pulse-slow': 'pulse 3s cubic-bezier(0.4, 0, 0.6, 1) infinite',
            }
,
            fontFamily: {
              sans: ["Inter", "sans-serif"],
              mono: ["Fira Code", "monospace"],
            },
            colors: {
              brand: {
                50: "#f5f3ff",
                100: "#ede9fe",
                200: "#ddd6fe",
                300: "#c4b5fd",
                400: "#a78bfa",
                500: "#8b5cf6", // Roxo principal
                600: "#7c3aed",
                700: "#6d28d9",
                800: "#5b21b6",
                900: "#4c1d95",
              },
            },
          },
        },
      };
    </script>
<style>
      /* Custom scrollbar */
      ::-webkit-scrollbar {
        width: 8px;
      }
      ::-webkit-scrollbar-track {
        background: #f1f1f1;
      }
      ::-webkit-scrollbar-thumb {
        background: #cbd5e1;
        border-radius: 4px;
      }
      ::-webkit-scrollbar-thumb:hover {
        background: #94a3b8;
      }

      .code-block {
        background-color: #1e1e1e;
        color: #d4d4d4;
        padding: 1.5rem;
        border-radius: 0.5rem;
        font-family: "Fira Code", monospace;
        overflow-x: auto;
        font-size: 0.85rem;
        line-height: 1.6;
        border: 1px solid #333;
      }

      .keyword {
        color: #569cd6;
      }
      .string {
        color: #ce9178;
      }
      .function {
        color: #dcdcaa;
      }
      .comment {
        color: #6a9955;
        font-style: italic;
      }

      .diagram-box {
        background: white;
        border: 2px solid #e5e7eb;
        border-radius: 8px;
        padding: 1rem;
        text-align: center;
        transition: all 0.3s ease;
        position: relative;
      }
      .diagram-box:hover {
        border-color: #7c3aed;
        box-shadow: 0 4px 6px rgba(124, 58, 237, 0.1);
        transform: translateY(-2px);
      }
      .arrow-right::after {
        content: "→";
        position: absolute;
        right: -20px;
        top: 50%;
        transform: translateY(-50%);
        color: #9ca3af;
        font-weight: bold;
      }
      @media (max-width: 768px) {
        .arrow-right::after {
          content: "↓";
          right: 50%;
          top: auto;
          bottom: -25px;
          transform: translateX(50%);
        }
      }

      /* Styles da Hero */
      /* Styles da Hero */
      .glass-card {
        background: rgba(255, 255, 255, 0.1);
        backdrop-filter: blur(12px);
        -webkit-backdrop-filter: blur(12px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        box-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.15);
      }

      .connection-line {
        position: relative;
        overflow: hidden;
        background: rgba(255, 255, 255, 0.15);
      }

      .connection-line::after {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: linear-gradient(
          90deg,
          transparent,
          rgba(255, 255, 255, 0.9),
          transparent
        );
        animation: flowLine 2s infinite linear;
      }

      .grid-bg {
        background-image: linear-gradient(
            rgba(255, 255, 255, 0.03) 1px,
            transparent 1px
          ),
          linear-gradient(90deg, rgba(255, 255, 255, 0.03) 1px, transparent 1px);
        background-size: 40px 40px;
      }
    </style>
</head>
<body class="bg-gray-50 text-gray-800 antialiased">
<!-- Header -->
<!-- Header -->
<header class="bg-white border-b border-gray-200 fixed w-full top-0 z-50">
<div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 h-16 flex items-center justify-between">
<div class="flex items-center space-x-3">
<div class="bg-brand-600 text-white p-2 rounded-lg">
<i class="fas fa-cubes"></i>
</div>
<h1 class="text-xl font-bold text-gray-900">
            LLM Product Engineering
          </h1>
</div>
<div class="hidden md:block text-sm text-gray-500">
          Série: Do Modelo ao Produto
        </div>
</div>
</header>
<div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 pt-24 pb-12 flex flex-col lg:flex-row gap-8">
<!-- Sidebar Navigation -->
<aside aria-label="Navegação lateral do artigo" class="lg:w-64 flex-shrink-0 hidden lg:block">
<nav class="sticky top-24 space-y-1">
<a class="group flex items-center px-3 py-2 text-sm font-medium rounded-md text-gray-600 hover:bg-brand-50 hover:text-brand-700" href="#intro">
<i class="fas fa-rocket w-6 text-brand-600"></i> Contexto
          </a>
<a class="group flex items-center px-3 py-2 text-sm font-medium rounded-md text-gray-600 hover:bg-gray-50 hover:text-gray-900" href="#approach">
<i class="fas fa-utensils w-6 text-brand-600"></i> Abordagem
          </a>
<a class="group flex items-center px-3 py-2 text-sm font-medium rounded-md text-gray-600 hover:bg-gray-50 hover:text-gray-900" href="#concepts">
<i class="fas fa-book w-6 text-brand-600"></i> Conceitos
          </a>
<a class="group flex items-center px-3 py-2 text-sm font-medium rounded-md text-gray-600 hover:bg-gray-50 hover:text-gray-900" href="#hands-on">
<i class="fas fa-code w-6 text-brand-600"></i> Mão na Massa
          </a>
<a class="group flex items-center px-3 py-2 text-sm font-medium rounded-md text-gray-600 hover:bg-gray-50 hover:text-gray-900" href="#risks">
<i class="fas fa-exclamation-triangle w-6 text-brand-600"></i>
            Riscos &amp; Métricas
          </a>
<a class="group flex items-center px-3 py-2 text-sm font-medium rounded-md text-gray-900 bg-brand-50 border-l-4 border-brand-600" href="#evidence">
<i class="fas fa-chart-bar w-6 text-brand-600"></i> Evidências
          </a>
<a class="group flex items-center px-3 py-2 text-sm font-medium rounded-md text-gray-600 hover:bg-gray-50 hover:text-gray-900" href="#reflections">
<i class="fas fa-lightbulb w-6 text-brand-600"></i> Reflexões
          </a>
</nav>
</aside>
<!-- Main Content -->
<main class="flex-1 space-y-12">
<!-- Hero Section (igual à capa) -->
<section class="relative rounded-3xl overflow-hidden mb-12 shadow-2xl">
<!-- Background Gradient & Grid -->
<div class="absolute inset-0 bg-gradient-to-br from-[#4c1d95] via-[#6d28d9] to-[#8b5cf6]"></div>
<div class="absolute inset-0 grid-bg opacity-30 pointer-events-none"></div>
<!-- Ambient Lighting (Decorativo) -->
<div class="absolute top-[-20%] left-[-20%] w-[60%] h-[60%] bg-brand-400 rounded-full mix-blend-overlay filter blur-[100px] opacity-20 animate-pulse-slow"></div>
<div class="absolute bottom-[-20%] right-[-20%] w-[60%] h-[60%] bg-brand-300 rounded-full mix-blend-overlay filter blur-[100px] opacity-20 animate-pulse-slow" style="animation-delay: 3s"></div>
<div class="flex flex-col lg:flex-row items-center justify-between px-8 py-16 md:px-16 md:py-20 relative z-10 gap-12">
<!-- Coluna Esquerda: Texto -->
<div class="lg:w-1/2 w-full text-white">
<div class="inline-flex items-center gap-2 px-3 py-1 rounded-full bg-white/10 border border-white/20 backdrop-blur-sm mb-6">
<i class="fas fa-microchip text-brand-200 text-xs"></i>
<span class="text-xs font-semibold tracking-wider uppercase text-brand-100">Tech Engineering</span>
</div>
<h1 class="text-4xl md:text-5xl lg:text-6xl font-extrabold tracking-tight leading-tight mb-6">
                Do modelo <br/>
<span class="text-transparent bg-clip-text bg-gradient-to-r from-brand-200 to-white">ao produto</span>
</h1>
<p class="text-lg md:text-xl text-brand-100 font-light leading-relaxed mb-8 border-l-4 border-brand-400 pl-6">
                Como transformar uma API de LLM em uma feature robusta,
                escalável e segura.
              </p>
</div>
<!-- Coluna Direita: Diagrama Visual (Model to Product) -->
<div class="lg:w-1/2 w-full flex items-center justify-center">
<div class="relative w-full max-w-md">
<div class="flex items-center justify-center gap-4">
<!-- Model (Brain) -->
<div class="flex flex-col items-center gap-2">
<div class="glass-card w-20 h-20 rounded-2xl flex items-center justify-center text-3xl text-brand-200 border border-brand-400/30 shadow-lg">
<i class="fas fa-brain"></i>
</div>
<span class="text-xs font-mono text-brand-200">MODEL</span>
</div>
<!-- Transformation Arrow -->
<div class="flex-1 h-1 bg-brand-400/30 rounded-full relative overflow-hidden">
<div class="absolute inset-0 bg-brand-200/50 w-1/2 animate-[flowLine_1.5s_infinite_linear]"></div>
</div>
<!-- API Layer (Middle) -->
<div class="flex flex-col items-center gap-2">
<div class="w-12 h-12 rounded-full bg-brand-500/20 flex items-center justify-center border border-brand-400 text-white text-xl animate-pulse">
<i class="fas fa-cog"></i>
</div>
</div>
<!-- Transformation Arrow -->
<div class="flex-1 h-1 bg-brand-400/30 rounded-full relative overflow-hidden">
<div class="absolute inset-0 bg-brand-200/50 w-1/2 animate-[flowLine_1.5s_infinite_linear]" style="animation-delay: 0.2s"></div>
</div>
<!-- Product (Box) -->
<div class="flex flex-col items-center gap-2">
<div class="glass-card w-20 h-20 rounded-2xl flex items-center justify-center text-3xl text-white border border-brand-300 shadow-[0_0_20px_rgba(139,92,246,0.4)]">
<i class="fas fa-cube"></i>
</div>
<span class="text-xs font-mono text-white font-bold">PRODUCT</span>
</div>
</div>
</div>
</div>
</div>
</section>
<!-- 1. Contexto -->
<section class="bg-white rounded-2xl shadow-sm p-8 border border-gray-100" id="intro">
<div class="prose max-w-none text-gray-600">
<p class="mb-7">
              Você já parou para pensar por que grandes apps de delivery parecem
              “inteligentes” ao resumir avaliações de restaurantes? Não é mágica
              — é engenharia sólida por trás de um LLM simples. Neste post, vou
              desmistificar essa jornada: de um protótipo rápido em Jupyter para
              uma feature robusta em produção, lidando com latência, custos e
              escalabilidade. Como dev full stack e DevOps, vi isso na prática
              em apps de alto tráfego.
            </p>
</div>
<h2 class="text-2xl font-bold text-gray-900 mb-6 border-b pb-4 flex items-center gap-2">
<span class="text-brand-600">1.</span> Contexto e Propósito
          </h2>
<div class="prose max-w-none text-gray-600">
<p class="mb-4">
              Integrar LLMs virou algo trivial: gere uma chave de API, escreva
              poucas linhas de código e voilà. Mas em um app processando
              milhares de pedidos por minuto, isso é só o começo. O grande
              desafio é transformar essa capacidade bruta em valor real para o
              usuário, sem quebrar o app com timeouts ou custos exorbitantes.
            </p>
<p class="mb-4">
<strong>O que você vai dominar:</strong> Aqui, compartilho a
              jornada de protótipo para produção, focando nos desafios de
              engenharia que todo dev precisa dominar para usar LLMs em escala.
            </p>
<ul class="mb-4 list-disc pl-6">
<li>Como garantir baixa latência em chamadas externas.</li>
<li>Estratégias para controlar custos de tokens.</li>
<li>A transição do notebook para o pipeline robusto.</li>
</ul>
<p class="mb-6">
<strong>Por que ler isso?</strong> Se você constrói features com
              IA, vai evitar armadilhas comuns que eu já cometi — e escalar de
              forma sustentável.
            </p>
<p class="mb-6 font-bold text-brand-700">Vamos mergulhar!</p>
<!-- CSS Diagram: Pipeline -->
<div class="bg-gray-50 p-6 rounded-xl border border-dashed border-gray-300">
<p class="text-center text-xs font-bold text-gray-400 uppercase tracking-widest mb-4">
                Pipeline de Produção
              </p>
<div class="flex flex-col md:flex-row justify-center items-center gap-8 md:gap-4">
<div class="diagram-box arrow-right md:w-32">
<i class="fas fa-flask text-brand-500 mb-2"></i><br/>Protótipo<br/><span class="text-xs text-gray-500">Jupyter</span>
</div>
<div class="diagram-box arrow-right md:w-32">
<i class="fas fa-code-branch text-brand-500 mb-2"></i><br/>Engenharia<br/><span class="text-xs text-gray-500">API &amp; Cache</span>
</div>
<div class="diagram-box md:w-32 border-brand-500 bg-brand-50">
<i class="fas fa-box-open text-brand-600 mb-2"></i><br/>Produto<br/><span class="text-xs text-gray-500">Feature Escalável</span>
</div>
</div>
</div>
</div>
</section>
<!-- 2. Abordagem -->
<section class="space-y-6" id="approach">
<div class="bg-white rounded-2xl shadow-sm p-8 border border-gray-100">
<h2 class="text-2xl font-bold text-gray-900 mb-6 border-b pb-4 flex items-center gap-2">
<span class="text-brand-600">2.</span> Abordagem Prática
            </h2>
<p class="text-gray-600 mb-6">
              Para tornar concreto, vamos usar um exemplo clássico:
              <strong>resumir avaliações de restaurantes</strong>. Em vez de
              listar dezenas de reviews, o app gera um parágrafo imparcial
              destacando prós e contras.
            </p>
<div class="grid md:grid-cols-2 gap-4">
<div class="bg-gray-50 p-4 rounded-lg border border-gray-200">
<h3 class="font-bold text-gray-800 mb-2">
<i class="fas fa-list-ul mr-2 text-brand-500"></i>Estrutura
                </h3>
<ul class="text-sm text-gray-600 space-y-2">
<li>• <strong>Conceitos:</strong> Modelo vs Feature.</li>
<li>• <strong>Arquitetura:</strong> Fluxo de dados.</li>
<li>• <strong>Código:</strong> Python/FastAPI.</li>
<li>• <strong>Produção:</strong> Riscos reais.</li>
</ul>
</div>
<div class="bg-blue-50 p-4 rounded-lg border border-blue-100">
<h3 class="font-bold text-blue-900 mb-2">
<i class="fas fa-bullseye mr-2"></i>Objetivo
                </h3>
<p class="text-sm text-blue-800">
                  Demonstrar como lidar com latência, custo e confiabilidade em
                  um cenário de alto tráfego.
                </p>
</div>
</div>
</div>
</section>
<!-- 3. Conceitos Fundamentais -->
<section class="bg-white rounded-2xl shadow-sm p-8 border border-gray-100" id="concepts">
<h2 class="text-2xl font-bold text-gray-900 mb-6 border-b pb-4 flex items-center gap-2">
<span class="text-brand-600">3.</span> Conceitos Fundamentais
          </h2>
<div class="overflow-x-auto">
<table class="min-w-full text-sm text-left border-collapse">
<thead class="bg-gray-100 text-gray-700 uppercase font-bold text-xs">
<tr>
<th class="px-6 py-3 rounded-tl-lg">Termo</th>
<th class="px-6 py-3">Definição</th>
<th class="px-6 py-3 rounded-tr-lg">
                    Exemplo prático / Dica
                  </th>
</tr>
</thead>
<tbody class="divide-y divide-gray-200">
<tr>
<td class="px-6 py-4 font-semibold text-brand-700">
                    LLM (Modelo)
                  </td>
<td class="px-6 py-4">
                    O "cérebro" (ex: GPT-4, Llama 3). Processa texto, não
                    armazena estado.
                  </td>
<td class="px-6 py-4 text-gray-600">
                    Recebe prompt e devolve resposta. Ex: "Resuma avaliações".
                  </td>
</tr>
<tr class="bg-gray-50">
<td class="px-6 py-4 font-semibold text-brand-700">Prompt</td>
<td class="px-6 py-4">Instrução/texto enviado ao LLM.</td>
<td class="px-6 py-4 text-gray-600">
                    "Resuma em 3 linhas: ...". Dica: seja específico!
                  </td>
</tr>
<tr>
<td class="px-6 py-4 font-semibold text-brand-700">Token</td>
<td class="px-6 py-4">
                    Unidade de texto processada pelo modelo (palavra ou pedaço).
                  </td>
<td class="px-6 py-4 text-gray-600">
                    Limite de tokens = limite de contexto. Cuidado com custos!
                  </td>
</tr>
<tr class="bg-gray-50">
<td class="px-6 py-4 font-semibold text-brand-700">
                    Context Window
                  </td>
<td class="px-6 py-4">
                    Quantidade máxima de tokens que o modelo "enxerga" por vez.
                  </td>
<td class="px-6 py-4 text-gray-600">
                    Foque nas últimas avaliações. Excedeu? Corte o texto.
                  </td>
</tr>
<tr>
<td class="px-6 py-4 font-semibold text-brand-700">
                    Feature
                  </td>
<td class="px-6 py-4">
                    Funcionalidade completa: código, API, cache, UI, etc.
                  </td>
<td class="px-6 py-4 text-gray-600">
                    Busca no DB + prompt + chamada LLM + exibição no app.
                  </td>
</tr>
<tr class="bg-gray-50">
<td class="px-6 py-4 font-semibold text-brand-700">Cache</td>
<td class="px-6 py-4">
                    Armazena respostas para evitar chamadas repetidas ao LLM.
                  </td>
<td class="px-6 py-4 text-gray-600">
                    Use Redis/Memcached. Dica: cacheie por restaurante/dia.
                  </td>
</tr>
<tr>
<td class="px-6 py-4 font-semibold text-brand-700">
                    Circuit Breaker
                  </td>
<td class="px-6 py-4">
                    Mecanismo para evitar sobrecarga/erros em chamadas externas.
                  </td>
<td class="px-6 py-4 text-gray-600">
                    Se o LLM falhar, devolva fallback (ex: reviews brutos).
                  </td>
</tr>
<tr class="bg-gray-50">
<td class="px-6 py-4 font-semibold text-brand-700">
                    Rate Limiting
                  </td>
<td class="px-6 py-4">
                    Limita o número de requisições por tempo.
                  </td>
<td class="px-6 py-4 text-gray-600">
                    Evita custos altos e abuso. Ex: 1 resumo/minuto por user.
                  </td>
</tr>
<tr>
<td class="px-6 py-4 font-semibold text-brand-700">
                    Observabilidade
                  </td>
<td class="px-6 py-4">
                    Monitoramento de métricas, logs e erros.
                  </td>
<td class="px-6 py-4 text-gray-600">
                    Meça latência, custo, taxa de erro. Use dashboards!
                  </td>
</tr>
</tbody>
</table>
</div>
<div class="mt-4 p-4 bg-yellow-50 border-l-4 border-yellow-400 text-sm text-yellow-800">
<strong>Mudança de Mindset:</strong> Pare de tratar o LLM como
            oráculo. Veja-o como um componente transformador de texto em um
            pipeline clássico.
          </div>
</section>
<!-- 4. Mão na Massa -->
<section class="bg-white rounded-2xl shadow-sm p-8 border border-gray-100" id="hands-on">
<h2 class="text-2xl font-bold text-gray-900 mb-6 border-b pb-4 flex items-center gap-2">
<span class="text-brand-600">4.</span> Mão na Massa: Implementação
          </h2>
<div class="prose max-w-none text-gray-700 mb-8">
<p>
              Agora vamos colocar a teoria em prática! Aqui você vê o fluxo real
              de uma feature de IA em um app de delivery: o cliente faz uma
              requisição, a API busca avaliações no banco, monta o prompt e
              chama o LLM para gerar o resumo. O diagrama abaixo mostra esse
              pipeline, e o código exemplifica como implementar esse endpoint
              usando FastAPI e OpenAI. Repare como cada etapa resolve um desafio
              citado nas seções anteriores: latência, custo e confiabilidade.
            </p>
</div>
<!-- Visual Flow -->
<div class="mb-8 p-6 bg-slate-50 rounded-xl border border-slate-200">
<div class="flex flex-wrap justify-center items-center gap-4 text-sm font-semibold text-slate-600">
<div class="bg-white p-3 rounded shadow-sm border border-slate-300">
                App (Client)
              </div>
<i class="fas fa-arrow-right text-slate-400"></i>
<div class="bg-white p-3 rounded shadow-sm border border-slate-300">
                Backend API
              </div>
<i class="fas fa-arrow-right text-slate-400"></i>
<div class="flex flex-col gap-2">
<div class="bg-brand-100 text-brand-700 p-2 rounded border border-brand-200 text-center text-xs">
                  DB (Reviews)
                </div>
<div class="bg-green-100 text-green-700 p-2 rounded border border-green-200 text-center text-xs">
                  OpenAI API
                </div>
</div>
</div>
</div>
<div class="mt-6 prose max-w-none text-gray-700 text-sm">
<ul class="list-disc pl-6">
<li>
<strong>App (Client):</strong> O usuário faz a requisição para
                ver o resumo das avaliações. Dica: garanta feedback visual
                enquanto espera a resposta, evitando sensação de lentidão.
              </li>
<li>
<strong>Backend API:</strong> Centraliza a lógica: busca dados,
                monta o prompt e chama o LLM. Desafio: proteger contra
                sobrecarga e garantir respostas rápidas. Boas práticas:
                implemente cache e circuit breaker.
              </li>
<li>
<strong>DB (Reviews):</strong> Fonte dos dados reais. Dica:
                filtre avaliações recentes e relevantes para não ultrapassar o
                limite de tokens do modelo.
              </li>
<li>
<strong>OpenAI API:</strong> Onde o resumo é gerado. Desafio:
                controlar custos e lidar com possíveis falhas. Boas práticas:
                monitore latência, limite chamadas e trate erros com fallback.
              </li>
</ul>
</div>
<div class="mt-6 mb-8 p-4 bg-yellow-50 border-l-4 border-yellow-400 text-sm text-yellow-900 rounded">
<strong>Armadilhas &amp; Boas Práticas:</strong><br/>
<ul class="list-disc pl-6">
<li>
<strong>Timeouts:</strong> Não trate o LLM como instantâneo.
                Implemente timeouts e feedback visual para o usuário.
              </li>
<li>
<strong>Excesso de tokens:</strong> Filtre e limite avaliações
                para não ultrapassar o contexto do modelo e evitar custos altos.
              </li>
<li>
<strong>Falhas de API:</strong> Sempre tenha um plano B: devolva
                reviews brutos ou uma mensagem amigável se o LLM falhar.
              </li>
<li>
<strong>Falta de cache:</strong> Sem cache, o app pode ficar
                lento e caro. Use Redis/Memcached para respostas repetidas.
              </li>
<li>
<strong>Monitoramento:</strong> Meça latência, taxa de erro e
                custo. Dashboards simples ajudam a detectar problemas antes do
                usuário.
              </li>
</ul>
<span class="block mt-2">Essas práticas diferenciam um protótipo de uma feature robusta e
              escalável.</span>
</div>
<div class="space-y-4 mb-8">
<h3 class="font-bold text-gray-800">Código: Endpoint FastAPI</h3>
<div class="prose max-w-none text-gray-700 text-sm mb-4">
<p>
<strong>Como funciona esse endpoint?</strong><br/>
                O código abaixo simula o fluxo completo de uma feature de IA em
                produção:
              </p>
<ul class="list-disc pl-6">
<li>
<strong>get_recent_reviews:</strong> Busca avaliações recentes
                  no banco. Na prática, você pode filtrar por data, relevância
                  ou quantidade para evitar excesso de tokens e garantir
                  contexto útil.
                </li>
<li>
<strong>generate_summary_prompt:</strong> Monta o prompt para
                  o LLM, organizando as avaliações de forma clara. Dica: seja
                  específico no prompt para evitar respostas genéricas.
                </li>
<li>
<strong>get_review_summary:</strong> Endpoint principal.
                  Orquestra todo o fluxo: busca dados, monta o prompt, chama o
                  LLM e trata erros. Note o uso de fallback (“Sem avaliações
                  suficientes”) e tratamento de exceções para garantir
                  confiabilidade.
                </li>
<li>
<strong>client.chat.completions.create:</strong> Chama o LLM
                  via API. Controle o modelo, max_tokens e temperature para
                  ajustar custo, latência e criatividade da resposta.
                </li>
</ul>
<p>
<strong>Dica de engenharia:</strong> Adapte esse endpoint para
                incluir cache, circuit breaker e monitoramento. Cada ajuste
                torna a feature mais robusta e pronta para escala.
              </p>
</div>
<pre class="code-block"><code><span class="keyword">import</span> os
<span class="keyword">from</span> typing <span class="keyword">import</span> List
<span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, HTTPException
<span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI

app = FastAPI()
client = OpenAI(api_key=os.getenv(<span class="string">"OPENAI_API_KEY"</span>))

<span class="comment"># Simulação de busca no banco</span>
<span class="keyword">def</span> <span class="function">get_recent_reviews</span>(restaurant_id: str) -&gt; List[str]:
    <span class="keyword">return</span> [
        <span class="string">"A comida chegou fria, mas estava saborosa."</span>,
        <span class="string">"Entrega super rápida! O sushi estava fresco."</span>
    ]

<span class="keyword">def</span> <span class="function">generate_summary_prompt</span>(reviews: List[str]) -&gt; str:
    reviews_text = <span class="string">"\n"</span>.join([<span class="string">f"- {r}"</span> <span class="keyword">for</span> r <span class="keyword">in</span> reviews])
    <span class="keyword">return</span> <span class="string">f"""
    Você é um assistente de delivery. Resuma em 3 linhas:
    Avaliações: {reviews_text}
    """</span>

<span class="keyword">@app.get</span>(<span class="string">"/restaurants/{restaurant_id}/reviews/summary"</span>)
<span class="keyword">async def</span> <span class="function">get_review_summary</span>(restaurant_id: str):
    <span class="keyword">try</span>:
        reviews = get_recent_reviews(restaurant_id)
        <span class="keyword">if not</span> reviews:
            <span class="keyword">return</span> {<span class="string">"summary"</span>: <span class="string">"Sem avaliações suficientes."</span>}

        prompt = generate_summary_prompt(reviews)

        <span class="comment"># Chamada ao LLM</span>
        response = client.chat.completions.create(
            model=<span class="string">"gpt-3.5-turbo"</span>,
            messages=[{<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: prompt}],
            max_tokens=150,
            temperature=0.5
        )

        <span class="keyword">return</span> {
            <span class="string">"restaurant_id"</span>: restaurant_id,
            <span class="string">"summary"</span>: response.choices[0].message.content.strip()
        }

    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:
        <span class="keyword">raise</span> HTTPException(status_code=503, detail=<span class="string">"Indisponível"</span>)</code></pre>
</div>
</section>
<!-- 5. Métricas e Riscos -->
<section class="bg-white rounded-2xl shadow-sm p-8 border border-gray-100" id="risks">
<h2 class="text-2xl font-bold text-gray-900 mb-6 border-b pb-4 flex items-center gap-2">
<span class="text-brand-600">5.</span> Métricas, Riscos e Boas
            Práticas
          </h2>
<div class="prose max-w-none text-gray-700 mb-6">
<p>
              Toda feature de IA em produção precisa ser acompanhada de perto:
              latência alta faz o usuário desistir, custos descontrolados podem
              inviabilizar o projeto e falhas de confiabilidade minam a
              confiança no app. Por isso, medir, monitorar e aplicar boas
              práticas não é luxo — é pré-requisito para escalar com segurança.
              Abaixo, veja os principais riscos e como mitigá-los na prática.
            </p>
</div>
<div class="grid md:grid-cols-3 gap-6 mb-8">
<!-- Card 1 -->
<div class="border-l-4 border-red-500 bg-red-50 p-4 rounded-r-lg">
<div class="flex items-center gap-2 mb-2 text-red-700 font-bold">
<i class="fas fa-stopwatch"></i> Latência
              </div>
<p class="text-xs text-red-800 mb-2">
                Chamadas acima de 2 segundos já fazem o usuário abandonar a tela
                ou perder confiança. LLMs podem variar muito em tempo de
                resposta, principalmente sob carga.
              </p>
<div class="text-xs bg-white p-2 rounded font-mono border border-red-200 mb-1">
                Solução: Cache (Redis) 24h.
              </div>
<p class="text-xs text-red-700">
<strong>Dica:</strong> Sempre meça o tempo de resposta e
                implemente feedback visual (ex: loading) no app. Use cache para
                respostas repetidas e reduza chamadas desnecessárias.
              </p>
</div>
<!-- Card 2 -->
<div class="border-l-4 border-yellow-500 bg-yellow-50 p-4 rounded-r-lg">
<div class="flex items-center gap-2 mb-2 text-yellow-700 font-bold">
<i class="fas fa-dollar-sign"></i> Custos
              </div>
<p class="text-xs text-yellow-800 mb-2">
                Cada chamada ao LLM tem custo por token. Em apps de alto
                tráfego, milhares de requisições podem gerar despesas
                inesperadas rapidamente.
              </p>
<div class="text-xs bg-white p-2 rounded font-mono border border-yellow-200 mb-1">
                Solução: Rate Limiting &amp; GPT-3.5
              </div>
<p class="text-xs text-yellow-700">
<strong>Dica:</strong> Implemente limites de uso por usuário e
                monitore o consumo. Prefira modelos mais baratos (ex: GPT-3.5)
                para tarefas simples e só use modelos avançados quando
                necessário.
              </p>
</div>
<!-- Card 3 -->
<div class="border-l-4 border-green-500 bg-green-50 p-4 rounded-r-lg">
<div class="flex items-center gap-2 mb-2 text-green-700 font-bold">
<i class="fas fa-shield-alt"></i> Confiabilidade
              </div>
<p class="text-xs text-green-800 mb-2">
                APIs externas podem ficar indisponíveis ou retornar respostas
                "alucinadas" (sem sentido). Isso afeta diretamente a experiência
                do usuário e a reputação do app.
              </p>
<div class="text-xs bg-white p-2 rounded font-mono border border-green-200 mb-1">
                Solução: Circuit Breaker
              </div>
<p class="text-xs text-green-700">
<strong>Dica:</strong> Sempre trate exceções e implemente
                fallback (ex: mostrar reviews brutos). Use circuit breaker para
                evitar sobrecarga e monitore erros para agir rápido.
              </p>
</div>
</div>
<div class="mt-4">
<div class="mb-4 p-4 bg-brand-50 border-l-4 border-brand-400 text-sm text-brand-900 rounded">
<strong>Resumo e convite:</strong> Métricas e boas práticas não
              são burocracia — são o que diferencia um protótipo de uma feature
              confiável e escalável. Monitore, ajuste e evolua sempre. Que tal
              revisar sua própria stack e identificar onde pode aplicar essas
              melhorias hoje mesmo?
            </div>
<div class="mb-4 p-4 bg-blue-50 border-l-4 border-blue-400 text-sm text-blue-900 rounded">
<strong>Exemplo prático:</strong> Imagine que seu app de delivery
              começa a crescer e, de repente, o custo com LLM dispara. Ao
              analisar as métricas, você percebe que 80% das chamadas são para
              os mesmos restaurantes. Ao implementar cache com Redis, o tempo de
              resposta cai de 3s para 200ms e o custo mensal reduz em 70%.
            </div>
<div class="p-4 bg-green-50 border-l-4 border-green-400 text-sm text-green-900 rounded">
<strong>Dica de engenharia:</strong> Sempre monitore as principais
              métricas (latência, custo, taxa de erro) com dashboards simples.
              Automatize alertas para detectar picos e aja rápido. Pequenas
              otimizações, como limitar tokens ou usar modelos mais baratos,
              fazem grande diferença em escala.
            </div>
</div>
</section>
<!-- 6. Evidências (FORMATAÇÃO CORRIGIDA) -->
<section class="bg-white rounded-2xl shadow-sm p-8 border border-gray-100" id="evidence">
<h2 class="text-2xl font-bold text-gray-900 mb-6 border-b pb-4 flex items-center gap-2">
<span class="text-brand-600">6.</span> Evidências &amp; Experimentação
          </h2>
<div class="prose max-w-none text-gray-700 mb-8">
<p>
              Não basta implementar uma feature de IA — é fundamental provar que
              ela realmente gera valor. Testes controlados, feedback dos
              usuários e monitoramento de métricas são o caminho para separar
              hype de resultado real. Aqui, mostramos como evidenciar impacto e
              aprender rápido com experimentação.
            </p>
</div>
<div class="space-y-8">
<!-- Teste 1: A/B Testing -->
<div class="bg-blue-50 rounded-xl p-6 border-l-4 border-blue-400">
<h3 class="font-bold text-lg mb-2 text-blue-900 flex items-center">
<i class="fas fa-vial mr-2"></i> Experimento 1: A/B Test na
                prática
              </h3>
<p class="text-sm text-blue-800 mb-4">
                Compare o impacto da feature de IA (resumo automático) com a
                versão tradicional (lista bruta de reviews). Separe usuários em
                dois grupos e meça resultados reais.
              </p>
<div class="grid md:grid-cols-2 gap-6">
<div>
<h4 class="text-xs font-bold uppercase text-blue-700 mb-2">
                    Métricas para comparar
                  </h4>
<ul class="text-sm space-y-2 text-blue-900">
<li>
<i class="fas fa-check text-green-500 mr-2"></i>CTR na
                      página
                    </li>
<li>
<i class="fas fa-check text-green-500 mr-2"></i>Tempo
                      médio gasto
                    </li>
<li>
<i class="fas fa-check text-green-500 mr-2"></i>Taxa de
                      conversão (+15-25% esperado)
                    </li>
</ul>
</div>
<div>
<h4 class="text-xs font-bold uppercase text-blue-700 mb-2">
                    Como analisar (SQL)
                  </h4>
<div class="code-block p-3 text-xs shadow-inner">
<pre><span class="keyword">SELECT</span> 
  variant, 
  <span class="function">COUNT</span>(*) <span class="keyword">AS</span> views,
  <span class="function">SUM</span>(converted) <span class="keyword">AS</span> conversions,
  (<span class="function">SUM</span>(converted) / <span class="function">COUNT</span>(*) * 100) <span class="keyword">AS</span> conv_rate
<span class="keyword">FROM</span> experiments
<span class="keyword">WHERE</span> feature = <span class="string">'ai_summary'</span>
<span class="keyword">GROUP BY</span> variant;</pre>
</div>
</div>
</div>
</div>
<!-- Teste 2: Feedback do Usuário -->
<div class="bg-green-50 rounded-xl p-6 border-l-4 border-green-400">
<h3 class="font-bold text-lg mb-2 text-green-900 flex items-center">
<i class="fas fa-comments mr-2"></i> Experimento 2: Feedback
                direto
              </h3>
<p class="text-sm text-green-800 mb-4">
                Implemente botões de "Útil" e "Não útil" para coletar feedback
                sobre o resumo gerado pela IA. Use esse dado para ajustar
                prompts ou ativar fallback automático.
              </p>
<div class="flex gap-3 mb-3">
<button class="bg-white px-4 py-2 rounded text-sm hover:bg-green-100 border border-green-200 text-green-700 font-medium transition-colors">
<i class="fas fa-thumbs-up mr-1"></i> Útil
                </button>
<button class="bg-white px-4 py-2 rounded text-sm hover:bg-red-50 border border-red-200 text-red-600 font-medium transition-colors">
<i class="fas fa-thumbs-down mr-1"></i> Não útil
                </button>
</div>
<p class="text-xs text-green-700 font-medium mt-3">
<i class="fas fa-lightbulb text-yellow-500 mr-1"></i> Insight:
                Se a taxa de "Não útil" passar de 10%, ative fallback para
                reviews brutos e revise o prompt.
              </p>
</div>
<!-- Teste 3: Monitoramento -->
<div class="bg-yellow-50 rounded-xl p-6 border-l-4 border-yellow-400">
<h3 class="font-bold text-lg mb-2 text-yellow-900 flex items-center">
<i class="fas fa-chart-line mr-2"></i> Experimento 3:
                Performance &amp; Custo
              </h3>
<p class="text-sm text-yellow-800 mb-4">
                Acompanhe diariamente o custo, a latência e a taxa de erro. Use
                dashboards simples para identificar gargalos.
              </p>
<div class="grid grid-cols-3 gap-2 text-center">
<div class="bg-white p-3 rounded border border-yellow-200">
<div class="text-xs text-gray-500 uppercase">Custo/Dia</div>
<div class="font-mono font-bold text-gray-800">$5.00</div>
</div>
<div class="bg-white p-3 rounded border border-yellow-200">
<div class="text-xs text-gray-500 uppercase">
                    Latência P95
                  </div>
<div class="font-mono font-bold text-red-500">2.3s</div>
</div>
<div class="bg-white p-3 rounded border border-yellow-200">
<div class="text-xs text-gray-500 uppercase">Erro Rate</div>
<div class="font-mono font-bold text-green-600">0.2%</div>
</div>
</div>
</div>
<div class="mt-4 p-4 bg-gray-50 border-l-4 border-gray-400 text-sm text-gray-600 rounded">
<strong>Conclusão:</strong> Não existe feature de IA perfeita sem
              experimentação. Teste, meça, ajuste e repita.
            </div>
</div>
</section>
<!-- 7. Reflexões & Próximos Passos (FORMATAÇÃO CORRIGIDA & ESTILIZADA) -->
<section class="bg-brand-900 rounded-2xl p-8 md:p-10 shadow-xl text-gray-200 border border-white/10" id="reflections">
<h2 class="text-2xl md:text-3xl font-bold text-white mb-6">
          7. Reflexões &amp; Próximos Passos
        </h2>
<div class="w-full h-px bg-white/10 mb-8"></div>
<div class="grid lg:grid-cols-12 gap-10 md:gap-16">
<div class="lg:col-span-7 space-y-6 text-lg leading-relaxed font-light">
<p>
              Como dev, aprendi que 90% do sucesso com IA é
              <strong class="text-brand-300 font-semibold">engenharia clássica</strong>: cache, monitoramento, resiliência. O prompt é o tempero. Sem
              base sólida, não escala.
            </p>
</div>
<div class="lg:col-span-5">
<h3 class="text-sm font-bold text-brand-300 uppercase tracking-widest mb-6 font-mono">
              Artigos Relacionados
            </h3>
<ul class="space-y-4">
<li class="flex items-start group">
<a class="flex items-center w-full hover:bg-white/5 p-2 -ml-2 rounded transition-colors" href="02-prompt-engineering-pace.html">
<span class="flex-shrink-0 w-8 h-8 flex items-center justify-center bg-white/10 rounded text-xs text-brand-300 mr-3">
<i class="fas fa-arrow-right"></i>
</span>
<div class="flex flex-col">
<span class="text-[10px] uppercase tracking-widest opacity-50 text-brand-300">Próximo</span>
<span class="text-sm text-gray-300 group-hover:text-white font-medium leading-tight">Prompt Engineering com PACE</span>
</div>
</a>
</li>
</ul>
</div>
</div>
</section>
</main>
</div>
<footer class="bg-white border-t border-gray-200 mt-12 py-8">
<div class="max-w-7xl mx-auto px-4 text-center text-gray-500 text-sm">
<p>
          © 2025 LLM Engineering Series. Construído com
          <i class="fas fa-heart text-red-400"></i> e IA.
        </p>
</div>
</footer>
</body>
</html>
